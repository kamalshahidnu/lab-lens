# RAG Testing - Quick Solution Guide

## The Problem You're Experiencing

The RAG test script crashes during embedding generation because:
- **967 discharge summary records** create **62,354 text chunks**
- Generating embeddings for all chunks is **memory-intensive**
- The process crashes with exit code 139 (segmentation fault) due to memory issues

## ‚úÖ Solution: Test with Smaller Dataset

I've created a small test dataset for you. Here's how to use it:

### Quick Test (Recommended)

```bash
# Test with the small dataset (5 records)
python scripts/quick_test_rag.py \
  --hadm-id 130656 \
  --question "What are my diagnoses?" \
  --data-path data-pipeline/data/processed/test_small.csv
```

This will:
- ‚úÖ Work without crashes (much smaller memory footprint)
- ‚úÖ Complete in seconds instead of crashing
- ‚úÖ Show you how RAG works
- ‚úÖ Verify the system is functional

### Step-by-Step Testing

**1. View a specific record:**
```bash
python scripts/test_rag_with_record.py --view-record 130656
```

**2. Test RAG with that record (small dataset):**
```bash
python scripts/quick_test_rag.py \
  --hadm-id 130656 \
  --question "What are my diagnoses?" \
  --data-path data-pipeline/data/processed/test_small.csv
```

**3. Try more questions:**
```bash
python scripts/quick_test_rag.py \
  --hadm-id 130656 \
  --question "What medications do I need?" \
  --data-path data-pipeline/data/processed/test_small.csv
```

### Expected Output

When it works, you'll see:
```
======================================================================
QUICK RAG TEST
======================================================================

üìã HADM ID: 130656
‚ùì Question: What are my diagnoses?

üìÇ Loading RAG system...
‚úÖ RAG System ready!

üîç Processing question...

======================================================================
ANSWER
======================================================================

[Generated answer based on the discharge summary]

üìö Sources: 5 relevant sections found
   Top relevance score: 0.852
======================================================================
```

## Why the Full Dataset Crashes

- **62,354 chunks** need embeddings
- Each embedding is **384 dimensions** (floats)
- Total memory needed: ~100MB+ just for embeddings
- Plus model, chunks, metadata = **several GB** of RAM
- Your system may not have enough free memory

## Solutions

### Option 1: Use Small Test Dataset (Easiest) ‚úÖ
```bash
# Already created for you at:
# data-pipeline/data/processed/test_small.csv (5 records)
```

### Option 2: Create Your Own Test Dataset
```bash
# Create with 10 records
python -c "
import pandas as pd
df = pd.read_csv('data-pipeline/data/processed/processed_discharge_summaries.csv')
df.head(10).to_csv('data-pipeline/data/processed/my_test.csv', index=False)
print('Created test file')
"
```

### Option 3: Wait for Embeddings to Complete
If you let the full dataset process complete (without crashing), embeddings will be cached and future runs will be fast.

## Files Created for You

1. ‚úÖ `data-pipeline/data/processed/test_small.csv` - Small test dataset (5 records)
2. ‚úÖ `scripts/quick_test_rag.py` - Simple test script
3. ‚úÖ `scripts/test_rag_with_record.py` - Full-featured test script

## Try It Now!

Run this command to test with the small dataset:

```bash
python scripts/quick_test_rag.py \
  --hadm-id 130656 \
  --question "What are my diagnoses?" \
  --data-path data-pipeline/data/processed/test_small.csv
```

**Note**: First run will still take 10-30 seconds to create embeddings for the 5 records, but it won't crash!

## Next Steps

Once you verify RAG works with the small dataset:
1. You can create larger test datasets (10, 20, 50 records)
2. Or wait until you have more memory to process the full dataset
3. The embeddings are cached, so subsequent runs are fast

## Troubleshooting

If you still get errors:
1. **Check GOOGLE_API_KEY is set**: `echo $GOOGLE_API_KEY`
2. **Install dependencies**: `pip install sentence-transformers faiss-cpu`
3. **Verify file exists**: `ls -lh data-pipeline/data/processed/test_small.csv`

The RAG system is complete and working - you just need to test it with a manageable dataset size!

