{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6492fb63",
   "metadata": {},
   "source": [
    "# MIMIC-III Data Acquisition Pipeline\n",
    "\n",
    "**Purpose:** This notebook acquires raw medical data from Google BigQuery's MIMIC-III database.\n",
    "\n",
    "**What it does:**\n",
    "- Connects to BigQuery and verifies access\n",
    "- Executes a comprehensive SQL query to extract:\n",
    "  - 5,000 discharge summaries (clinical notes)\n",
    "  - Patient demographics (age, gender, ethnicity, insurance)\n",
    "  - Lab results and abnormal flags\n",
    "  - Diagnosis codes (ICD-9)\n",
    "- Saves everything to a single CSV file: `mimic_discharge_labs.csv`\n",
    "\n",
    "**What it does NOT do:**\n",
    "- Data cleaning (handled by preprocessing script)\n",
    "- Data validation (handled by validation script)\n",
    "- Bias detection (handled by bias detection script)\n",
    "\n",
    "**Output:** `data/raw/mimic_discharge_labs.csv` (~95 MB, ~9,700 records, 18 columns)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea22cafa",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries\n",
    "\n",
    "We need:\n",
    "- `bigquery`: To query Google's MIMIC-III database\n",
    "- `InstalledAppFlow`: For interactive Google authentication\n",
    "- `pandas`: To handle the resulting data\n",
    "- `os` & `json`: For file paths and configuration management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "127f5b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Import libraries\n",
    "from google.cloud import bigquery\n",
    "from google_auth_oauthlib.flow import InstalledAppFlow\n",
    "import pandas as pd\n",
    "import os\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af6bee69",
   "metadata": {},
   "source": [
    "## 2. Load Configuration & Setup Paths\n",
    "\n",
    "**Why this matters:**\n",
    "- We use a `pipeline_config.json` file to specify where data should be saved\n",
    "- This makes the notebook **portable** - it works on any machine without hardcoding paths\n",
    "- The script automatically creates the `data/raw/` directory if it doesn't exist\n",
    "\n",
    "**What happens here:**\n",
    "1. Find the project root (2 directories up from `notebooks/`)\n",
    "2. Load `pipeline_config.json`\n",
    "3. Convert the relative path from config into an absolute path\n",
    "4. Create the directory if needed\n",
    "\n",
    "**Result:** The variable `RAW_DATA_DIR` now points to the correct location to save our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3a5b861c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Project root: /Users/Admin/Desktop/lab-lens\n",
      "✓ Data directory: /Users/Admin/Desktop/lab-lens/data-pipeline/data/raw\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Setup - Load config and convert relative paths to absolute\n",
    "# Find project root (2 levels up from notebooks/)\n",
    "PROJECT_ROOT = os.path.abspath(os.path.join(os.getcwd(), '../..'))\n",
    "\n",
    "# Load config\n",
    "config_path = os.path.join(PROJECT_ROOT, 'data-pipeline/configs/pipeline_config.json')\n",
    "with open(config_path, 'r') as f:\n",
    "    config = json.load(f)\n",
    "\n",
    "# Convert relative config path to absolute path\n",
    "RAW_DATA_DIR = os.path.join(PROJECT_ROOT, config['pipeline_config']['input_path'])\n",
    "os.makedirs(RAW_DATA_DIR, exist_ok=True)\n",
    "\n",
    "print(f\"✓ Project root: {PROJECT_ROOT}\")\n",
    "print(f\"✓ Data directory: {RAW_DATA_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7911f9f0",
   "metadata": {},
   "source": [
    "## 3. Authenticate with Google BigQuery\n",
    "\n",
    "**What happens:**\n",
    "1. A browser window opens asking you to log in to Google\n",
    "2. You grant permission for this script to access BigQuery\n",
    "3. The script receives credentials and creates a `client` object\n",
    "\n",
    "**Security note:** \n",
    "- The `client_id` and `client_secret` shown here are for the Google OAuth app, not your personal credentials\n",
    "- Your actual credentials are stored locally and never shared\n",
    "\n",
    "**Project:** 'Project_ID' (our GCP project with access to MIMIC-III)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "180497c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please visit this URL to authorize this application: https://accounts.google.com/o/oauth2/auth?response_type=code&client_id=764086051850-6qr4p6gpi6hn506pt8ejuq83di341hur.apps.googleusercontent.com&redirect_uri=http%3A%2F%2Flocalhost%3A8081%2F&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fbigquery&state=NUikgHvo8PSxpT5ETsunoIjsBe1ukI&prompt=consent&access_type=offline\n",
      "✓ Connected to BigQuery!\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: Authenticate with BigQuery\n",
    "flow = InstalledAppFlow.from_client_config(\n",
    "    {\n",
    "        \"installed\": {\n",
    "            \"client_id\": \"764086051850-6qr4p6gpi6hn506pt8ejuq83di341hur.apps.googleusercontent.com\",\n",
    "            \"client_secret\": \"d-FL95Q19q7MQmFpd7hHD0Ty\",\n",
    "            \"auth_uri\": \"https://accounts.google.com/o/oauth2/auth\",\n",
    "            \"token_uri\": \"https://oauth2.googleapis.com/token\",\n",
    "            \"redirect_uris\": [\"http://localhost\"]\n",
    "        }\n",
    "    },\n",
    "    scopes=[\"https://www.googleapis.com/auth/bigquery\"]\n",
    ")\n",
    "\n",
    "credentials = flow.run_local_server(port=8081, prompt='consent')\n",
    "client = bigquery.Client(project='regal-bonito-455919-u3', credentials=credentials)\n",
    "print(\"✓ Connected to BigQuery!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b97dc34b",
   "metadata": {},
   "source": [
    "## 4. Verify Connection to MIMIC-III Database\n",
    "\n",
    "**Purpose:** Before running expensive queries, we verify the connection works.\n",
    "\n",
    "**What we're checking:**\n",
    "- Can we connect to BigQuery?\n",
    "- Do we have access to the MIMIC-III database?\n",
    "- Is the `admissions` table accessible?\n",
    "\n",
    "**Expected result:** 58,976 total admissions in the database\n",
    "\n",
    "**Why this matters:** \n",
    "- BigQuery charges per query, so we want to catch connection issues early\n",
    "- If this fails, we know there's an authentication or permissions problem before wasting time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f7486478",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Connection verified: 58,976 total admissions in database\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: Test connection with simple query\n",
    "test_query = \"\"\"\n",
    "SELECT COUNT(*) as count\n",
    "FROM `physionet-data.mimiciii_clinical.admissions`\n",
    "\"\"\"\n",
    "\n",
    "try:\n",
    "    df_test = client.query(test_query).to_dataframe()\n",
    "    print(f\"✓ Connection verified: {df_test['count'][0]:,} total admissions in database\")\n",
    "except Exception as e:\n",
    "    print(f\"✗ Connection failed: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a884a77",
   "metadata": {},
   "source": [
    "## 5. Document Available Tables\n",
    "\n",
    "**Purpose:** List all tables in the `mimiciii_clinical` dataset for reference.\n",
    "\n",
    "**Key tables we'll use:**\n",
    "- `admissions` - Hospital admission records\n",
    "- `patients` - Patient demographics (gender, date of birth)\n",
    "- `labevents` - Laboratory test results\n",
    "- `diagnoses_icd` - ICD-9 diagnosis codes\n",
    "- `d_labitems` - Lab test definitions (what each test means)\n",
    "\n",
    "**Why list them:** \n",
    "- Documentation for future queries\n",
    "- Verify we have access to all required tables\n",
    "- Quick reference without checking BigQuery console"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "33f4e39b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available tables in mimiciii_clinical:\n",
      "  - admissions\n",
      "  - callout\n",
      "  - caregivers\n",
      "  - chartevents\n",
      "  - cptevents\n",
      "  - d_cpt\n",
      "  - d_icd_diagnoses\n",
      "  - d_icd_procedures\n",
      "  - d_items\n",
      "  - d_labitems\n",
      "  - datetimeevents\n",
      "  - diagnoses_icd\n",
      "  - drgcodes\n",
      "  - icustays\n",
      "  - inputevents_cv\n",
      "  - inputevents_mv\n",
      "  - labevents\n",
      "  - microbiologyevents\n",
      "  - outputevents\n",
      "  - patients\n",
      "  - prescriptions\n",
      "  - procedureevents_mv\n",
      "  - procedures_icd\n",
      "  - services\n",
      "  - transfers\n"
     ]
    }
   ],
   "source": [
    "# Cell 5: List available tables in mimiciii_clinical\n",
    "print(\"Available tables in mimiciii_clinical:\")\n",
    "tables = client.list_tables('physionet-data.mimiciii_clinical')\n",
    "for table in tables:\n",
    "    print(f\"  - {table.table_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ce13c5b",
   "metadata": {},
   "source": [
    "## 6. Verify Access to All Required Datasets\n",
    "\n",
    "**MIMIC-III is split into 4 datasets:**\n",
    "1. `mimiciii_clinical` - Structured clinical data (labs, admissions, diagnoses)\n",
    "2. `mimiciii_notes` - Clinical notes (discharge summaries, radiology reports)\n",
    "3. `mimiciii_derived` - Pre-calculated metrics\n",
    "4. `mimiciii_notes_derived` - Processed note data\n",
    "\n",
    "**What we're checking:**\n",
    "- ✓ Do we have access to `mimiciii_clinical`? (for demographics, labs, diagnoses)\n",
    "- ✓ Do we have access to `mimiciii_notes`? (for discharge summaries)\n",
    "\n",
    "**Why this matters:**\n",
    "- Our query needs BOTH datasets\n",
    "- If either is missing, the main query will fail\n",
    "- This \"fail-fast\" approach saves time by catching issues before the expensive query runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ab76ae95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available MIMIC-III datasets:\n",
      "  ✓ mimiciii_clinical\n",
      "  ✓ mimiciii_derived\n",
      "  ✓ mimiciii_notes\n",
      "  ✓ mimiciii_notes_derived\n"
     ]
    }
   ],
   "source": [
    "# Cell 6: Verify access to all MIMIC-III datasets\n",
    "datasets = list(client.list_datasets('physionet-data'))\n",
    "mimic_datasets = [d for d in datasets if 'mimic' in d.dataset_id.lower()]\n",
    "\n",
    "print(\"Available MIMIC-III datasets:\")\n",
    "for dataset in mimic_datasets:\n",
    "    print(f\"  ✓ {dataset.dataset_id}\")\n",
    "\n",
    "# Verify required datasets exist\n",
    "required = ['mimiciii_clinical', 'mimiciii_notes']\n",
    "available = [d.dataset_id for d in mimic_datasets]\n",
    "for req in required:\n",
    "    if req not in available:\n",
    "        raise ValueError(f\"Required dataset '{req}' not found!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2e2c8aa",
   "metadata": {},
   "source": [
    "## 7. Execute Comprehensive Data Acquisition Query\n",
    "\n",
    "**This is the main data extraction.** One complex SQL query joins multiple tables to create our complete dataset.\n",
    "\n",
    "### The Query Has 4 Parts (CTEs):\n",
    "\n",
    "#### Part 1: `discharge` CTE\n",
    "- Extracts 5,000 discharge summaries from `noteevents`\n",
    "- Removes PHI placeholders: `[**Name**]` → (blank)\n",
    "- Calculates text length for each note\n",
    "\n",
    "#### Part 2: `demographics` CTE\n",
    "- Joins `patients` and `admissions` tables\n",
    "- Extracts: gender, ethnicity, insurance, language, admission type\n",
    "- Calculates: age at admission, length of stay\n",
    "\n",
    "#### Part 3: `labs` CTE\n",
    "- Aggregates lab results from `labevents`\n",
    "- Creates a summary string of up to 20 lab values per patient\n",
    "- Flags abnormal results with `(!)`\n",
    "- Counts total labs and abnormal labs\n",
    "\n",
    "#### Part 4: `diagnoses` CTE\n",
    "- Aggregates diagnosis codes (ICD-9) per admission\n",
    "- Counts total diagnoses\n",
    "- Lists up to 10 diagnosis codes per patient\n",
    "\n",
    "### Final Join:\n",
    "- Combines all 4 parts using `hadm_id` (hospital admission ID)\n",
    "- Uses LEFT JOIN to keep all discharge summaries, even if labs/diagnoses are missing\n",
    "\n",
    "### Expected Result:\n",
    "- ~9,700 records (some patients have multiple admissions)\n",
    "- 18 columns covering demographics, clinical notes, labs, and diagnoses\n",
    "- Average note length: ~10,500 characters\n",
    "\n",
    "**Why one big query?**\n",
    "- More efficient than multiple queries\n",
    "- BigQuery charges per query, so this saves money\n",
    "- Ensures data consistency (all from the same point in time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ee29eddd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running comprehensive acquisition query...\n",
      "This may take several minutes...\n",
      "\n",
      "✓ Query completed successfully\n",
      "✓ Loaded 9,651 records\n",
      "✓ Average text length: 10429 characters\n"
     ]
    }
   ],
   "source": [
    "# Cell 7: Execute comprehensive acquisition query\n",
    "query = r\"\"\"\n",
    "WITH discharge AS (\n",
    "    SELECT \n",
    "        n.hadm_id,\n",
    "        n.subject_id,\n",
    "        REGEXP_REPLACE(n.text, r'\\[\\*\\*[^\\]]*\\*\\*\\]', '') AS cleaned_text,\n",
    "        LENGTH(n.text) as text_length\n",
    "    FROM `physionet-data.mimiciii_notes.noteevents` n\n",
    "    WHERE n.category = 'Discharge summary'\n",
    "    LIMIT 5000\n",
    "),\n",
    "demographics AS (\n",
    "    SELECT \n",
    "        p.subject_id,\n",
    "        p.gender,\n",
    "        a.ethnicity,\n",
    "        a.insurance,\n",
    "        a.language,\n",
    "        a.marital_status,\n",
    "        a.admission_type,\n",
    "        a.admittime,\n",
    "        a.dischtime,\n",
    "        DATETIME_DIFF(a.admittime, p.dob, YEAR) as age_at_admission,\n",
    "        DATETIME_DIFF(a.dischtime, a.admittime, DAY) as length_of_stay\n",
    "    FROM `physionet-data.mimiciii_clinical.patients` p\n",
    "    JOIN `physionet-data.mimiciii_clinical.admissions` a \n",
    "        ON p.subject_id = a.subject_id\n",
    "),\n",
    "labs AS (\n",
    "    SELECT \n",
    "        le.hadm_id,\n",
    "        STRING_AGG(\n",
    "            CONCAT(d.label, ': ', CAST(le.value AS STRING), ' ', le.valueuom,\n",
    "            CASE WHEN le.flag = 'abnormal' THEN ' (!)' ELSE '' END), '; '\n",
    "            LIMIT 20\n",
    "        ) AS lab_summary,\n",
    "        COUNT(*) as total_labs,\n",
    "        SUM(CASE WHEN le.flag = 'abnormal' THEN 1 ELSE 0 END) AS abnormal_count\n",
    "    FROM `physionet-data.mimiciii_clinical.labevents` le\n",
    "    JOIN `physionet-data.mimiciii_clinical.d_labitems` d ON le.itemid = d.itemid\n",
    "    GROUP BY le.hadm_id\n",
    "),\n",
    "diagnoses AS (\n",
    "    SELECT \n",
    "        hadm_id,\n",
    "        COUNT(*) as diagnosis_count,\n",
    "        STRING_AGG(icd9_code, ', ' LIMIT 10) as top_diagnoses\n",
    "    FROM `physionet-data.mimiciii_clinical.diagnoses_icd`\n",
    "    GROUP BY hadm_id\n",
    ")\n",
    "SELECT \n",
    "    d.*,\n",
    "    dem.gender,\n",
    "    dem.ethnicity,\n",
    "    dem.age_at_admission,\n",
    "    dem.insurance,\n",
    "    dem.language,\n",
    "    dem.admission_type,\n",
    "    dem.admittime,\n",
    "    dem.dischtime,\n",
    "    dem.length_of_stay,\n",
    "    l.lab_summary,\n",
    "    l.total_labs,\n",
    "    l.abnormal_count,\n",
    "    diag.diagnosis_count,\n",
    "    diag.top_diagnoses\n",
    "FROM discharge d\n",
    "LEFT JOIN demographics dem ON d.subject_id = dem.subject_id\n",
    "LEFT JOIN labs l ON d.hadm_id = l.hadm_id\n",
    "LEFT JOIN diagnoses diag ON d.hadm_id = diag.hadm_id\n",
    "\"\"\"\n",
    "\n",
    "print(\"Running comprehensive acquisition query...\")\n",
    "print(\"This may take several minutes...\\n\")\n",
    "\n",
    "try:\n",
    "    df_complete = client.query(query).to_dataframe()\n",
    "    print(f\"✓ Query completed successfully\")\n",
    "    print(f\"✓ Loaded {len(df_complete):,} records\")\n",
    "    print(f\"✓ Average text length: {df_complete['text_length'].mean():.0f} characters\")\n",
    "except Exception as e:\n",
    "    print(f\"✗ Query failed: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdbcd112",
   "metadata": {},
   "source": [
    "## 8. Save Raw Data to CSV\n",
    "\n",
    "**What happens:**\n",
    "1. DataFrame is written to: `data/raw/mimic_discharge_labs.csv`\n",
    "2. File size is calculated and displayed\n",
    "3. Confirmation message shows the exact file location\n",
    "\n",
    "**Expected output:**\n",
    "- File size: ~95 MB\n",
    "- Records: ~9,700\n",
    "- Columns: 18\n",
    "\n",
    "**Error handling:** If save fails (disk full, permission denied), the script will stop and show the error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "62db271f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Data saved successfully\n",
      "✓ Location: /Users/Admin/Desktop/lab-lens/data-pipeline/data/raw/mimic_discharge_labs.csv\n",
      "✓ File size: 93.33 MB\n"
     ]
    }
   ],
   "source": [
    "# Cell 8: Save raw data\n",
    "output_path = os.path.join(RAW_DATA_DIR, 'mimic_discharge_labs.csv')\n",
    "\n",
    "try:\n",
    "    df_complete.to_csv(output_path, index=False)\n",
    "    file_size_mb = os.path.getsize(output_path) / 1024 / 1024\n",
    "    print(f\"✓ Data saved successfully\")\n",
    "    print(f\"✓ Location: {output_path}\")\n",
    "    print(f\"✓ File size: {file_size_mb:.2f} MB\")\n",
    "except Exception as e:\n",
    "    print(f\"✗ Save failed: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b5f357fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hadm_id', 'subject_id', 'cleaned_text', 'text_length', 'gender', 'ethnicity', 'age_at_admission', 'insurance', 'language', 'admission_type', 'admittime', 'dischtime', 'length_of_stay', 'lab_summary', 'total_labs', 'abnormal_count', 'diagnosis_count', 'top_diagnoses']\n"
     ]
    }
   ],
   "source": [
    "print(df_complete.columns.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff14548c",
   "metadata": {},
   "source": [
    "## 9. Acquisition Verification Report\n",
    "\n",
    "**Purpose:** Confirm the data was acquired correctly before moving to the next pipeline stage.\n",
    "\n",
    "### What We're Verifying:\n",
    "\n",
    "#### ✓ Basic Metrics\n",
    "- Total records acquired\n",
    "- Number of columns\n",
    "- File size on disk\n",
    "\n",
    "#### ✓ Schema Validation\n",
    "We check for all 18 expected columns:\n",
    "\n",
    "**Identifiers:**\n",
    "- `hadm_id`, `subject_id`\n",
    "\n",
    "**Clinical Text:**\n",
    "- `cleaned_text`, `text_length`\n",
    "\n",
    "**Demographics:**\n",
    "- `gender`, `ethnicity`, `age_at_admission`, `insurance`, `language`, `admission_type`\n",
    "\n",
    "**Temporal Data:**\n",
    "- `admittime`, `dischtime`, `length_of_stay`\n",
    "\n",
    "**Lab Data:**\n",
    "- `lab_summary`, `total_labs`, `abnormal_count`\n",
    "\n",
    "**Diagnosis Data:**\n",
    "- `diagnosis_count`, `top_diagnoses`\n",
    "\n",
    "#### ✓ Column Validation\n",
    "- ✗ Missing columns → Indicates query problem\n",
    "- ⚠ Extra columns → Indicates schema drift\n",
    "\n",
    "### If Everything Passes:\n",
    "✓ **Acquisition complete! Ready for preprocessing.**\n",
    "\n",
    "The data is now ready for the next pipeline stage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "71715f4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "ACQUISITION VERIFICATION REPORT\n",
      "============================================================\n",
      "\n",
      "✓ Records acquired: 9,651\n",
      "✓ Columns: 18\n",
      "✓ File size: 93.33 MB\n",
      "\n",
      "✓ Schema:\n",
      "  ✓ hadm_id\n",
      "  ✓ subject_id\n",
      "  ✓ cleaned_text\n",
      "  ✓ text_length\n",
      "  ✓ gender\n",
      "  ✓ ethnicity\n",
      "  ✓ age_at_admission\n",
      "  ✓ insurance\n",
      "  ✓ language\n",
      "  ✓ admission_type\n",
      "  ✓ admittime\n",
      "  ✓ dischtime\n",
      "  ✓ length_of_stay\n",
      "  ✓ lab_summary\n",
      "  ✓ total_labs\n",
      "  ✓ abnormal_count\n",
      "  ✓ diagnosis_count\n",
      "  ✓ top_diagnoses\n",
      "\n",
      "============================================================\n",
      "✓ Acquisition complete! Ready for preprocessing.\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Cell 9: Verify acquisition completed successfully\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ACQUISITION VERIFICATION REPORT\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Basic metrics\n",
    "print(f\"\\n✓ Records acquired: {len(df_complete):,}\")\n",
    "print(f\"✓ Columns: {len(df_complete.columns)}\")\n",
    "print(f\"✓ File size: {os.path.getsize(output_path)/1024/1024:.2f} MB\")\n",
    "\n",
    "# Schema check\n",
    "print(f\"\\n✓ Schema:\")\n",
    "expected_columns = [\n",
    "    'hadm_id', 'subject_id', 'cleaned_text', 'text_length',\n",
    "    'gender', 'ethnicity', 'age_at_admission', 'insurance', 'language',\n",
    "    'admission_type', 'admittime', 'dischtime', 'length_of_stay',\n",
    "    'lab_summary', 'total_labs', 'abnormal_count',\n",
    "    'diagnosis_count', 'top_diagnoses'\n",
    "]\n",
    "\n",
    "for col in expected_columns:\n",
    "    status = \"✓\" if col in df_complete.columns else \"✗\"\n",
    "    print(f\"  {status} {col}\")\n",
    "\n",
    "# Missing columns check\n",
    "missing = set(expected_columns) - set(df_complete.columns)\n",
    "if missing:\n",
    "    print(f\"\\n⚠ Warning: Missing columns: {missing}\")\n",
    "\n",
    "# Extra columns check\n",
    "extra = set(df_complete.columns) - set(expected_columns)\n",
    "if extra:\n",
    "    print(f\"\\n⚠ Warning: Unexpected columns: {extra}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"✓ Acquisition complete! Ready for preprocessing.\")\n",
    "print(\"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
